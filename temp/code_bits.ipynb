{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(requests.get(\"https://coinmarketcap.com/\").text, \"lxml\")\n",
    "dom = soup.find(string=\"Dominance\").parent.parent.text\n",
    "dom = unicodedata.normalize(\"NFKD\", dom)\n",
    "dom\n",
    "\n",
    "matches = re.findall(r\"(\\w+): (\\d+\\.\\d+)%\", dom)\n",
    "\n",
    "# Create dictionaries to store the percentages\n",
    "percentage_dict = {}\n",
    "\n",
    "for match in matches:\n",
    "    currency, percentage = match\n",
    "    percentage_dict[currency] = float(percentage)\n",
    "\n",
    "print(percentage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "def smooth_and_detect_peaks_multi(\n",
    "    df: pd.DataFrame,\n",
    "    window_daily: int = 12,  # 2 weeks for daily data\n",
    "    polyorder: int = 2,\n",
    "    peak_threshold: float = 0.15,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process multiple tokens with mixed daily/hourly data.\n",
    "    Adds smoothed price and peak detection columns for each token.\n",
    "    \"\"\"\n",
    "\n",
    "    def rolling_savgol(series: pd.Series, window: int) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Forward-only rolling Savitzky-Golay filter.\n",
    "        At each point t, only uses data from [t-window+1 : t].\n",
    "\n",
    "        Parameters:\n",
    "        - series: Price series\n",
    "        - window: Window size\n",
    "        - polyorder: Polynomial order\n",
    "        \"\"\"\n",
    "        values = series.values\n",
    "        result = np.full_like(values, np.nan)\n",
    "\n",
    "        # First few points just use the original values\n",
    "        result[: polyorder + 2] = values[: polyorder + 2]\n",
    "\n",
    "        # For each point, fit polynomial to previous window points\n",
    "        for i in tqdm(range(polyorder + 2, len(values))):\n",
    "            # Get window of past data\n",
    "            start_idx = max(0, i - window + 1)\n",
    "            window_data = values[start_idx : i + 1]\n",
    "\n",
    "            if len(window_data) > polyorder + 1:\n",
    "                try:\n",
    "                    # Fit polynomial to window\n",
    "                    x = np.arange(len(window_data))\n",
    "                    coeffs = np.polyfit(x, window_data, polyorder)\n",
    "\n",
    "                    # Use polynomial value at last point\n",
    "                    result[i] = np.polyval(coeffs, len(window_data) - 1)\n",
    "                except:\n",
    "                    # If fit fails, use original value\n",
    "                    result[i] = values[i]\n",
    "            else:\n",
    "                result[i] = values[i]\n",
    "\n",
    "        return pd.Series(result, index=series.index)\n",
    "\n",
    "    def detect_peaks(smooth_prices: pd.Series, raw_prices: pd.Series, window: int) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Detect peaks and troughs using find_peaks with minimal lookahead\n",
    "        \"\"\"\n",
    "        signals = pd.Series(0, index=smooth_prices.index)\n",
    "        prices = smooth_prices.values\n",
    "\n",
    "        # Parameters for find_peaks\n",
    "        lookahead = 1  # Maximum forward-looking window\n",
    "        distance = 3  # Minimum samples between peaks\n",
    "        prominence = 0.0005  # Minimum prominence relative to neighbors\n",
    "\n",
    "        for i in range(window, len(prices) - lookahead):\n",
    "            # Get local window of prices\n",
    "            local_window = prices[max(0, i - window) : i + lookahead + 1]\n",
    "            center_idx = min(window, i)\n",
    "\n",
    "            # Adjust prominence based on local volatility\n",
    "            local_std = np.std(local_window) / np.mean(local_window)\n",
    "            adaptive_prominence = max(prominence, local_std * 0.5)\n",
    "\n",
    "            if window >= 480:  # For hourly data\n",
    "                distance = 4\n",
    "\n",
    "            # Find peaks in local window\n",
    "            peak_indices, peak_props = find_peaks(local_window, distance=distance, prominence=adaptive_prominence)\n",
    "\n",
    "            # Find troughs in inverted window\n",
    "            trough_indices, trough_props = find_peaks(-local_window, distance=distance, prominence=adaptive_prominence)\n",
    "\n",
    "            # Check if center point is a peak or trough\n",
    "            if len(peak_indices) > 0 and center_idx in peak_indices:\n",
    "                # Confirm peak with slope check\n",
    "                if all(prices[i] > prices[i - a] for a in range(1, lookahead)) and all(\n",
    "                    prices[i] > prices[i + a] for a in range(1, lookahead)\n",
    "                ):\n",
    "                    signals.iloc[i] = 1\n",
    "\n",
    "            elif len(trough_indices) > 0 and center_idx in trough_indices:\n",
    "                # Confirm trough with slope check\n",
    "                if all(prices[i] < prices[i - a] for a in range(1, lookahead)) and all(\n",
    "                    prices[i] < prices[i + a] for a in range(1, lookahead)\n",
    "                ):\n",
    "                    signals.iloc[i] = -1\n",
    "\n",
    "        return signals\n",
    "\n",
    "    def process_token(price_series: pd.Series) -> tuple[pd.Series, pd.Series]:\n",
    "        \"\"\"Process single token with mixed frequency data\"\"\"\n",
    "\n",
    "        # Rest of the processing...\n",
    "        time_deltas = price_series.index.to_series().diff()\n",
    "        is_hourly = time_deltas <= pd.Timedelta(hours=1)\n",
    "        is_daily = ~is_hourly\n",
    "\n",
    "        window_hourly = window_daily * 24\n",
    "\n",
    "        print(price_series[is_hourly].index[0], price_series[is_hourly].index[-1])\n",
    "        print(price_series[is_daily].index[0], price_series[is_daily].index[-1])\n",
    "\n",
    "        # Apply smoothing with appropriate window\n",
    "        smooth_hourly = rolling_savgol(price_series[is_hourly], window_hourly)\n",
    "        smooth_daily = rolling_savgol(price_series[is_daily], window_daily)\n",
    "\n",
    "        # Detect peaks with appropriate window\n",
    "        peaks_hourly = detect_peaks(smooth_hourly, price_series[is_hourly], window_hourly)\n",
    "        peaks_daily = detect_peaks(smooth_daily, price_series[is_daily], window_daily)\n",
    "\n",
    "        # Combine daily and hourly data\n",
    "        smooth_combined = pd.concat([smooth_daily, smooth_hourly]).sort_index()\n",
    "        peaks_combined = pd.concat([peaks_daily, peaks_hourly]).sort_index()\n",
    "\n",
    "        # Drop duplicate indices\n",
    "        smooth_combined = smooth_combined[~smooth_combined.index.duplicated(keep=\"last\")]\n",
    "        peaks_combined = peaks_combined[~peaks_combined.index.duplicated(keep=\"last\")]\n",
    "\n",
    "        return smooth_combined, peaks_combined\n",
    "\n",
    "    # df = df.copy()\n",
    "\n",
    "    # Process each token\n",
    "    token_columns = [col for col in df.columns if col.endswith(\"_price\")]\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remove duplicates from main DataFrame index\n",
    "    df = df[~df.index.duplicated(keep=\"last\")]\n",
    "\n",
    "    for price_col in token_columns:\n",
    "        token = price_col.replace(\"_price\", \"\")\n",
    "\n",
    "        print(f\"Processing {token}...\")\n",
    "\n",
    "        smooth_series, peaks_series = process_token(df[price_col])\n",
    "\n",
    "        # Align indices before assignment\n",
    "        df[f\"{token}_smooth\"] = smooth_series.reindex(df.index)\n",
    "        df[f\"{token}_peaks\"] = peaks_series.reindex(df.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "savgol_df = smooth_and_detect_peaks_multi(new_master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def add_token_indicators(\n",
    "    df: pd.DataFrame,\n",
    "    base_metrics=[\"price\", \"total_volume\"],\n",
    "    market_cols=[\n",
    "        \"total_market_cap\",\n",
    "        \"total_volume\",\n",
    "        \"alt_market_cap\",\n",
    "        \"alt_dominance\",\n",
    "        \"alt_fgindex\",\n",
    "    ],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds technical indicators for each token, ensuring no forward-looking bias.\n",
    "\n",
    "    Args:\n",
    "            df: DataFrame with columns in format {token}_{metric}\n",
    "            base_metrics: List of base metrics to process\n",
    "    Returns:\n",
    "            DataFrame with ordered columns and indicators\n",
    "    \"\"\"\n",
    "\n",
    "    def expanding_min_max_scale(\n",
    "        series: pd.Series,\n",
    "        method: str = \"min_max\",\n",
    "        # clip_quantile: bool = False,\n",
    "        clip_on_min: bool = False,\n",
    "        clip_value: float = None,\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Rolling scaling using only past data\n",
    "        \"\"\"\n",
    "        if method == \"min_max\":\n",
    "            expanding_max = series.expanding().max()\n",
    "            expanding_min = series.expanding().min()\n",
    "            if clip_value and not clip_on_min:\n",
    "                quantile = series.quantile(clip_value)\n",
    "                expanding_max = expanding_max.clip(upper=quantile)\n",
    "            if clip_value and clip_on_min:\n",
    "                quantile = series.quantile(clip_value)\n",
    "                expanding_min = expanding_min.clip(lower=quantile)\n",
    "\n",
    "            denominator = expanding_max - expanding_min\n",
    "\n",
    "            scaled_col = np.where(\n",
    "                denominator != 0,\n",
    "                np.clip((series - expanding_min) * 100 / denominator, 0, 100),\n",
    "                0,\n",
    "            )\n",
    "        elif method == \"percentile\":\n",
    "            expanding_percentile = series.expanding().apply(\n",
    "                lambda x: pd.Series(x).rank(pct=True).iloc[-1] * 100, raw=False\n",
    "            )\n",
    "            scaled_col = expanding_percentile\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose 'min_max' or 'percentile'.\")\n",
    "\n",
    "        return scaled_col\n",
    "\n",
    "    def get_first_hourly_index(series: pd.Series) -> pd.Timestamp:\n",
    "        \"\"\"Find the first hourly index in the Series\"\"\"\n",
    "        time_diffs = series.index.to_series().diff()\n",
    "        hour_index = time_diffs[time_diffs < pd.Timedelta(days=1)].index[0]\n",
    "        hour_idx = series.index.get_loc(hour_index)\n",
    "        return hour_idx\n",
    "\n",
    "    def calculate_roc(series: pd.Series, window: int) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Vectorized ROC calculation handling both daily and hourly data\n",
    "\n",
    "        Args:\n",
    "                series: Input time series\n",
    "                window: Window size in days\n",
    "        Returns:\n",
    "                Rate of change series\n",
    "        \"\"\"\n",
    "        result = pd.Series(index=series.index, dtype=float)\n",
    "        hour_loc = get_first_hourly_index(series)\n",
    "\n",
    "        # Daily data calculation\n",
    "        daily_slice = slice(None, hour_loc)\n",
    "        result.iloc[daily_slice] = series.iloc[daily_slice] / series.iloc[daily_slice].shift(window) - 1\n",
    "\n",
    "        # Hourly data calculation\n",
    "        hourly_slice = slice(hour_loc, None)\n",
    "        hourly_window = window * 24\n",
    "        result.iloc[hourly_slice] = series.iloc[hourly_slice] / series.iloc[hourly_slice].shift(hourly_window) - 1\n",
    "\n",
    "        return result.fillna(0)\n",
    "\n",
    "        # print(get_first_hourly_index(series))\n",
    "        # dp(series.iloc[:get_first_hourly_index(series)], series.iloc[get_first_hourly_index(series):])\n",
    "\n",
    "        # return (series / series.shift(window) - 1).fillna(0)\n",
    "\n",
    "    def calculate_acceleration(series: pd.Series, short_window: int, long_window: int) -> pd.Series:\n",
    "        \"\"\"Vectorized acceleration calculation\"\"\"\n",
    "        return (calculate_roc(series, short_window) - calculate_roc(series, long_window)).fillna(0)\n",
    "\n",
    "    # Extract unique tokens\n",
    "    tokens: Set[str] = {col.split(\"_\")[0] for col in df.columns if \"price\" in col}\n",
    "\n",
    "    # Initialize new DataFrame to maintain column order\n",
    "    result_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Process each token\n",
    "    for token in tokens:\n",
    "        token_cols = []  # Track columns for this token\n",
    "\n",
    "        # Base metrics\n",
    "        for metric in base_metrics:\n",
    "            col_name = f\"{token}_{metric}\"\n",
    "            if col_name in df.columns:\n",
    "                result_df[col_name] = df[col_name]\n",
    "                token_cols.append(col_name)\n",
    "\n",
    "        # Price indicators\n",
    "        if f\"{token}_price\" in df.columns:\n",
    "            price = df[f\"{token}_price\"]\n",
    "\n",
    "            # Core price indicators\n",
    "            indicators = {\n",
    "                \"price_scaled\": expanding_min_max_scale(price),\n",
    "                # \"price_roc_14d\": calculate_roc(price, 14),\n",
    "                # \"price_roc_30d\": calculate_roc(price, 30),\n",
    "                # \"price_accel\": calculate_acceleration(price, 14, 30),\n",
    "            }\n",
    "\n",
    "            # Add indicators to result\n",
    "            for suffix, values in indicators.items():\n",
    "                col_name = f\"{token}_{suffix}\"\n",
    "                result_df[col_name] = values\n",
    "                token_cols.append(col_name)\n",
    "\n",
    "        # Volume indicators\n",
    "        if f\"{token}_total_volume\" in df.columns:\n",
    "            volume = df[f\"{token}_total_volume\"]\n",
    "\n",
    "            # Core volume indicators\n",
    "            indicators = {\n",
    "                \"volume_scaled\": expanding_min_max_scale(volume, clip_value=0.995, clip_on_min=False),\n",
    "                # \"volume_roc_14d\": calculate_roc(volume, 14),\n",
    "                # \"price_roc_30d\": calculate_roc(volume, 30),\n",
    "                # \"price_accel\": calculate_acceleration(price, 14, 30),\n",
    "            }\n",
    "\n",
    "            # Add indicators to result\n",
    "            for suffix, values in indicators.items():\n",
    "                col_name = f\"{token}_{suffix}\"\n",
    "                result_df[col_name] = values\n",
    "                token_cols.append(col_name)\n",
    "\n",
    "        # Reorder columns for this token\n",
    "        result_df = result_df.reindex(\n",
    "            columns=[col for col in result_df.columns if col not in token_cols] + sorted(token_cols)\n",
    "        )\n",
    "\n",
    "    # Market-wide indicators\n",
    "    # if len(tokens) > 1:\n",
    "    # market_cols = []\n",
    "\n",
    "    # Total market volume\n",
    "    # total_volume = sum(\n",
    "    #     df[f\"{token}_total_volume\"]\n",
    "    #     for token in tokens\n",
    "    #     if f\"{token}_total_volume\" in df.columns\n",
    "    # )\n",
    "\n",
    "    for col in market_cols:\n",
    "        result_df[col] = df[col]\n",
    "        if col == \"alt_dominance\":\n",
    "            result_df[f\"{col}_scaled\"] = expanding_min_max_scale(df[col], clip_value=0.01, clip_on_min=True)\n",
    "        else:\n",
    "            result_df[f\"{col}_scaled\"] = expanding_min_max_scale(df[col])\n",
    "\n",
    "    result_df[\"alt_dominance_roc_30d\"] = calculate_roc(df[\"alt_dominance\"], 30)\n",
    "\n",
    "    # result_df[\"market_total_volume\"] = total_volume\n",
    "    # result_df[\"market_total_volume_scaled\"] = expanding_min_max_scale(total_volume)\n",
    "    # market_cols.extend([\"market_total_volume\", \"market_total_volume_scaled\"])\n",
    "\n",
    "    # Volume dominance\n",
    "    # for token in tokens:\n",
    "    #     if f\"{token}_total_volume\" in df.columns:\n",
    "    #         col_name = f\"{token}_volume_dominance\"\n",
    "    #         result_df[col_name] = (\n",
    "    #             df[f\"{token}_total_volume\"] / total_volume\n",
    "    #         ).fillna(0)\n",
    "    #         market_cols.append(col_name)\n",
    "\n",
    "    # # Move market columns to the end\n",
    "    # non_market_cols = [col for col in result_df.columns if col not in market_cols]\n",
    "    # result_df = result_df.reindex(columns=non_market_cols + sorted(market_cols))\n",
    "\n",
    "    cols_order = []\n",
    "    for token in tokens:\n",
    "        col_suffixes = [\n",
    "            f\"{token}_{suffix}\"\n",
    "            for suffix in [\n",
    "                \"price\",\n",
    "                \"price_scaled\",\n",
    "                # \"price_roc_14d\",\n",
    "                # \"price_roc_30d\",\n",
    "                # \"price_accel\",\n",
    "                \"total_volume\",\n",
    "                \"volume_scaled\",\n",
    "                # \"volume_roc_14d\",\n",
    "            ]\n",
    "        ]\n",
    "        cols_order.extend(col_suffixes)\n",
    "\n",
    "    for col in market_cols:\n",
    "        cols_order.extend([col, f\"{col}_scaled\"])\n",
    "\n",
    "    cols_order.insert(cols_order.index(\"alt_dominance_scaled\") + 1, \"alt_dominance_roc_30d\")\n",
    "\n",
    "    result_df = result_df[cols_order]\n",
    "\n",
    "    # Clean up any remaining infinities or NaNs\n",
    "    # result_df = result_df.replace([np.inf, -np.inf], np.nan)\n",
    "    # result_df = result_df.fillna(method=\"ffill\").fillna(0)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "new_master_df = add_token_indicators(master_df)\n",
    "new_master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Create boolean mask for rows where empyreal price is null/nan/none\n",
    "# Create boolean mask where empyreal price is valid (1) or NA (0)\n",
    "def plot_data_availability(df, col_name):\n",
    "    \"\"\"\n",
    "    Plot data availability for a given column in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data\n",
    "    col_name (str): The column name to check for data availability\n",
    "    \"\"\"\n",
    "    data_mask = (~df[col_name].isna()).astype(int)\n",
    "\n",
    "    # Plot the mask values with datetime index\n",
    "    fig = px.line(\n",
    "        x=df.index,\n",
    "        y=data_mask,\n",
    "        title=f\"{col_name} Data Availability (1=Valid, 0=NA)\",\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Data Available\",\n",
    "        yaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "token_names = list(set([col.split(\"_\")[0] for col in df.columns]))\n",
    "\n",
    "for token in token_names:\n",
    "    plot_data_availability(df, f\"{token}_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# -- Align and merge dataframes --\n",
    "\n",
    "\n",
    "def align_and_merge_dataframes(\n",
    "    primary_df: pd.DataFrame,\n",
    "    secondary_dfs: list[tuple[pd.DataFrame, list[str], dict[str, str]]],\n",
    "    ffill_limit: int = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Align and merge multiple dataframes with a primary dataframe based on datetime index.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    primary_df : pd.DataFrame\n",
    "            The primary dataframe with datetime index to which others will be aligned\n",
    "    secondary_dfs : list of tuples\n",
    "            List of (dataframe, columns_to_include, rename_dict) tuples where:\n",
    "            - dataframe: DataFrame to be merged\n",
    "            - columns_to_include: List of column names to include\n",
    "            - rename_dict: Dictionary mapping original column names to new names\n",
    "    ffill_limit : int, optional\n",
    "            Maximum number of consecutive NaN values to forward fill\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "            Merged dataframe with aligned datetime index\n",
    "    \"\"\"\n",
    "    # Ensure primary df has datetime index and is sorted\n",
    "    if not isinstance(primary_df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Primary DataFrame index must be DatetimeIndex\")\n",
    "\n",
    "    result_df = primary_df.copy().sort_index()\n",
    "\n",
    "    # Process each secondary dataframe\n",
    "    for df, cols, rename_dict in secondary_dfs:\n",
    "        # Validate columns exist\n",
    "        if missing_cols := set(cols) - set(df.columns):\n",
    "            raise ValueError(f\"Columns {missing_cols} not found in secondary DataFrame\")\n",
    "\n",
    "        # Process secondary dataframe\n",
    "        processed_df = df[cols].rename(columns=rename_dict).sort_index().reindex(result_df.index)\n",
    "\n",
    "        # Forward fill if specified\n",
    "        # if ffill_limit is not None:\n",
    "        #     processed_df = processed_df.ffill(limit=ffill_limit)\n",
    "\n",
    "        # Merge with result\n",
    "        result_df = pd.concat([result_df, processed_df], axis=1)\n",
    "        result_df[[col for col in rename_dict.values()]] = result_df[[col for col in rename_dict.values()]].ffill(\n",
    "            limit=ffill_limit\n",
    "        )\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"token_data.csv\", index_col=0, parse_dates=True)\n",
    "dp(df)\n",
    "master_df = align_and_merge_dataframes(\n",
    "    df,\n",
    "    [\n",
    "        # (fg_df, [\"score\"], {\"score\": \"fgindex\"}),\n",
    "        (alt_fgindex, [\"value\"], {\"value\": \"alt_fgindex\"}),\n",
    "        (\n",
    "            mcap_df,\n",
    "            [\"total_market_cap\", \"total_volume\", \"alt_market_cap\", \"alt_dominance\"],\n",
    "            {\n",
    "                \"total_market_cap\": \"total_market_cap\",\n",
    "                \"total_volume\": \"total_volume\",\n",
    "                \"alt_market_cap\": \"alt_market_cap\",\n",
    "                \"alt_dominance\": \"alt_dominance\",\n",
    "            },\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def smooth_and_detect_peaks_multi(\n",
    "    df: pd.DataFrame,\n",
    "    window_daily: int = 20,\n",
    "    polyorder: int = 3,\n",
    "    lookahead: int = 3\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combined smoothing and peak detection using forward-looking Savitzky-Golay\n",
    "    \"\"\"\n",
    "    def rolling_savgol_with_peaks(series: pd.Series, window: int) -> tuple[pd.Series, pd.Series]:\n",
    "        \"\"\"\n",
    "        Rolling Savitzky-Golay with peak detection in single pass\n",
    "        Returns (smooth_series, peaks_series)\n",
    "        \"\"\"\n",
    "        values = series.values\n",
    "        smooth_result = np.full_like(values, np.nan)\n",
    "        peaks_result = np.full_like(values, 0)\n",
    "\n",
    "        # Determine optimal lookbehind window\n",
    "        time_deltas = series.index.to_series().diff()\n",
    "        is_hourly = time_deltas.median() <= pd.Timedelta(hours=1)\n",
    "\n",
    "        # For hourly data, use larger lookbehind to capture daily patterns\n",
    "        # For daily data, use smaller lookbehind to maintain responsiveness\n",
    "        lookbehind = window if is_hourly else window // 2\n",
    "        prominence = 0.00001 if is_hourly else 0.0001\n",
    "\n",
    "        min_points = polyorder + 2\n",
    "        smooth_result[:min_points] = values[:min_points]\n",
    "\n",
    "        for i in tqdm(range(min_points, len(values) - lookahead)):\n",
    "            try:\n",
    "                # Get extended window\n",
    "                start_idx = max(0, i - lookbehind)\n",
    "                window_data = values[start_idx:i + lookahead + 1]\n",
    "\n",
    "                if len(window_data) > polyorder + 1:\n",
    "                    x = np.arange(len(window_data))\n",
    "                    coeffs = np.polyfit(x, window_data, polyorder)\n",
    "\n",
    "                    # Get all smoothed points in window\n",
    "                    smoothed_window = np.polyval(coeffs, x)\n",
    "\n",
    "                    # Store smoothed value for current point\n",
    "                    # Use the point that corresponds to current position in window\n",
    "                    current_pos = i - start_idx\n",
    "                    smooth_result[i] = smoothed_window[current_pos]\n",
    "\n",
    "                    # Peak detection on local window\n",
    "                    if current_pos >= 2 and current_pos < len(smoothed_window) - 2:\n",
    "                        local_std = np.std(smoothed_window) / np.mean(smoothed_window)\n",
    "                        adaptive_prominence = max(prominence, local_std * 0.1)\n",
    "                        adaptive_prominence\n",
    "\n",
    "                        # Check for peak\n",
    "                        if (smoothed_window[current_pos] > smoothed_window[current_pos - 1] and\n",
    "                            smoothed_window[current_pos] > smoothed_window[current_pos + 1] and\n",
    "                            smoothed_window[current_pos] == max(smoothed_window[current_pos - 2:current_pos + 3])):\n",
    "                            if abs(smoothed_window[current_pos] - smoothed_window[current_pos - 1]) > adaptive_prominence:\n",
    "                                peaks_result[i] = 1\n",
    "\n",
    "                        # Check for trough\n",
    "                        elif (smoothed_window[current_pos] < smoothed_window[current_pos - 1] and\n",
    "                            smoothed_window[current_pos] < smoothed_window[current_pos + 1] and\n",
    "                            smoothed_window[current_pos] == min(smoothed_window[current_pos - 2:current_pos + 3])):\n",
    "                            if abs(smoothed_window[current_pos] - smoothed_window[current_pos - 1]) > adaptive_prominence:\n",
    "                                peaks_result[i] = -1\n",
    "\n",
    "                else:\n",
    "                    smooth_result[i] = values[i]\n",
    "\n",
    "            except Exception:\n",
    "                smooth_result[i] = values[i]\n",
    "\n",
    "        return (pd.Series(smooth_result, index=series.index),\n",
    "                pd.Series(peaks_result, index=series.index))\n",
    "\n",
    "    def process_token(price_series: pd.Series) -> tuple[pd.Series, pd.Series]:\n",
    "        \"\"\"Process single token\"\"\"\n",
    "        time_deltas = price_series.index.to_series().diff()\n",
    "        is_hourly = time_deltas <= pd.Timedelta(hours=1)\n",
    "        is_daily = ~is_hourly\n",
    "\n",
    "        window_hourly = window_daily * 24\n",
    "\n",
    "        # Process hourly and daily data\n",
    "        if len(price_series[is_hourly]) > 0:\n",
    "            smooth_hourly, peaks_hourly = rolling_savgol_with_peaks(\n",
    "                price_series[is_hourly], window_hourly)\n",
    "        else:\n",
    "            smooth_hourly = peaks_hourly = pd.Series()\n",
    "\n",
    "        if len(price_series[is_daily]) > 0:\n",
    "            smooth_daily, peaks_daily = rolling_savgol_with_peaks(\n",
    "                price_series[is_daily], window_daily)\n",
    "        else:\n",
    "            smooth_daily = peaks_daily = pd.Series()\n",
    "\n",
    "        # Combine results\n",
    "        smooth_combined = pd.concat([smooth_daily, smooth_hourly]).sort_index()\n",
    "        peaks_combined = pd.concat([peaks_daily, peaks_hourly]).sort_index()\n",
    "\n",
    "        return (smooth_combined[~smooth_combined.index.duplicated(keep=\"last\")],\n",
    "                peaks_combined[~peaks_combined.index.duplicated(keep=\"last\")])\n",
    "\n",
    "    # Main processing\n",
    "    df = df.copy()[~df.index.duplicated(keep=\"last\")]\n",
    "\n",
    "    for price_col in [col for col in df.columns if col.endswith(\"_price\")]:\n",
    "        token = price_col.replace(\"_price\", \"\")\n",
    "        smooth_series, peaks_series = process_token(df[price_col])\n",
    "\n",
    "        df[f\"{token}_smooth\"] = smooth_series.reindex(df.index)\n",
    "        df[f\"{token}_peaks\"] = peaks_series.reindex(df.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "savgol_df = smooth_and_detect_peaks_multi(new_master_df.filter(like = 'swap'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# df = pd.read_csv(\"token_data.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "\n",
    "# def reindex_to_daily(df, cutoff_datetime):\n",
    "#     \"\"\"\n",
    "#     Reindex the DataFrame to daily frequency for all dates prior to the given datetime.\n",
    "\n",
    "#     Parameters:\n",
    "#     df (pd.DataFrame): The DataFrame to reindex\n",
    "#     cutoff_datetime (datetime): The datetime value to use as the cutoff\n",
    "\n",
    "#     Returns:\n",
    "#     pd.DataFrame: The reindexed DataFrame\n",
    "#     \"\"\"\n",
    "#     # Ensure the index is in datetime format\n",
    "#     #df.index = pd.to_datetime(df.index)\n",
    "\n",
    "#     # Split the DataFrame into two parts: before and after the cutoff datetime\n",
    "#     before_cutoff = df[df.index < cutoff_datetime]\n",
    "#     after_cutoff = df[df.index >= cutoff_datetime]\n",
    "\n",
    "#     # Reindex the 'before_cutoff' part to daily frequency\n",
    "#     before_cutoff = before_cutoff[before_cutoff.index.time == pd.Timestamp(\"00:00:00\").time()]\n",
    "\n",
    "#     # Concatenate the two parts back together\n",
    "#     reindexed_df = pd.concat([before_cutoff, after_cutoff]).sort_index()\n",
    "\n",
    "#     return reindexed_df\n",
    "\n",
    "# # Example usage\n",
    "# df_ = reindex_to_daily(df, '2023-11-20 08:00:00')\n",
    "\n",
    "# df_.to_csv(\"token_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "def plot_token_signals(df: pd.DataFrame, tokens: list[str] = None, days_back: int = 180) -> go.Figure:\n",
    "\n",
    "    \"\"\"\n",
    "    Plot price, smoothed price, and peaks/troughs for multiple tokens\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with token data\n",
    "    - tokens: List of token names (without _price suffix)\n",
    "    - days_back: Number of days to plot\n",
    "    \"\"\"\n",
    "    if tokens == None:\n",
    "        tokens = list(set(col.replace(\"_price\", \"\") for col in df.columns if col.endswith(\"_price\")))\n",
    "\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=len(tokens),\n",
    "        cols=1,\n",
    "        subplot_titles=[t.upper() for t in tokens],\n",
    "        vertical_spacing=0.05,\n",
    "    )\n",
    "\n",
    "    # Plot each token\n",
    "    for idx, token in enumerate(tokens, 1):\n",
    "        # Get recent data\n",
    "        plot_df = df\n",
    "\n",
    "        # Add price line\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=plot_df.index,\n",
    "                y=plot_df[f\"{token}_price\"],\n",
    "                name=f\"{token} Price\",\n",
    "                line=dict(color=\"lightgray\"),\n",
    "                showlegend=idx == 1,\n",
    "            ),\n",
    "            row=idx,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        # Add smoothed line\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=plot_df.index,\n",
    "                y=plot_df[f\"{token}_smooth\"],\n",
    "                name=\"Smoothed Price\",\n",
    "                line=dict(color=\"blue\"),\n",
    "                showlegend=idx == 1,\n",
    "            ),\n",
    "            row=idx,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        # Add peaks\n",
    "        peaks = plot_df[plot_df[f\"{token}_peaks\"] == 1]\n",
    "        if len(peaks) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=peaks.index,\n",
    "                    y=peaks[f\"{token}_price\"],\n",
    "                    mode=\"markers\",\n",
    "                    name=\"Peaks\",\n",
    "                    marker=dict(symbol=\"triangle-down\", size=10, color=\"red\"),\n",
    "                    showlegend=idx == 1,\n",
    "                ),\n",
    "                row=idx,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "        # Add troughs\n",
    "        troughs = plot_df[plot_df[f\"{token}_peaks\"] == -1]\n",
    "        if len(troughs) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=troughs.index,\n",
    "                    y=troughs[f\"{token}_price\"],\n",
    "                    mode=\"markers\",\n",
    "                    name=\"Troughs\",\n",
    "                    marker=dict(symbol=\"triangle-up\", size=10, color=\"green\"),\n",
    "                    showlegend=idx == 1,\n",
    "                ),\n",
    "                row=idx,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=300 * len(tokens),\n",
    "        title_text=\"Token Prices with Peak Detection\",\n",
    "        showlegend=True,\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "    )\n",
    "\n",
    "    # Update y-axes to log scale\n",
    "    fig.update_yaxes(type=\"log\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "plot_token_signals(savgol_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# -- Plotting the indicators --\n",
    "\n",
    "\n",
    "# Code to plot a list of columns in plotly and indicating whether they are a primary or secondary y axis\n",
    "def plot_columns(df, columns, secondary_y_cols=None, marker_col=None):\n",
    "    \"\"\"\n",
    "    Plot specified columns with primary and secondary y-axes, and add markers for peaks and troughs.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data\n",
    "    columns (list): List of column names to plot\n",
    "    secondary_y_cols (list): List of column names to plot on secondary y-axis\n",
    "    marker_col (str): Column name for markers (+1 for peaks, -1 for troughs)\n",
    "    \"\"\"\n",
    "    if secondary_y_cols is None:\n",
    "        secondary_y_cols = []\n",
    "\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    for col in columns:\n",
    "        if col in secondary_y_cols:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=df.index, y=df[col], name=col, mode=\"lines\"),\n",
    "                secondary_y=True,\n",
    "            )\n",
    "        else:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=df.index, y=df[col], name=col, mode=\"lines\"),\n",
    "                secondary_y=False,\n",
    "            )\n",
    "\n",
    "    if marker_col and marker_col in df.columns:\n",
    "        peaks = df[(df[marker_col] == 1) & (df[f\"{token}_price\"] != 0)]\n",
    "        troughs = df[(df[marker_col] == -1) & (df[f\"{token}_price\"] != 0)]\n",
    "\n",
    "        secondary_y = False\n",
    "        if f\"{token}_price\" in secondary_y_cols:\n",
    "            secondary_y = True\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=peaks.index,\n",
    "                y=peaks[f\"{token}_price\"],\n",
    "                mode=\"markers\",\n",
    "                name=\"Peaks\",\n",
    "                marker=dict(symbol=\"triangle-down\", size=10, color=\"red\"),\n",
    "            ),\n",
    "            secondary_y=secondary_y,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=troughs.index,\n",
    "                y=troughs[f\"{token}_price\"],\n",
    "                mode=\"markers\",\n",
    "                name=\"Troughs\",\n",
    "                marker=dict(symbol=\"triangle-up\", size=10, color=\"green\"),\n",
    "            ),\n",
    "            secondary_y=secondary_y,\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Plot of Specified Columns\",\n",
    "        xaxis_title=\"Date\",\n",
    "        hovermode=\"x unified\",\n",
    "        showlegend=True,\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(title_text=\"Primary Y-Axis\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Secondary Y-Axis\", secondary_y=True)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# columns_to_plot = [\"alt_dominance\", \"alt_dominance_scaled\", \"alt_fgindex\", \"alt_fgindex_scaled\"]\n",
    "columns_to_plot = [\n",
    "    \"alt_dominance\",\n",
    "    \"alt_dominance_scaled\",\n",
    "    # \"alt_fgindex_scaled\",\n",
    "    \"alt_market_cap\",\n",
    "    # \"total_market_cap_scaled\",\n",
    "    \"alt_market_cap_scaled\",\n",
    "    # \"alt_dominance_roc_30d\",\n",
    "]\n",
    "# columns_to_plot = []\n",
    "secondary_y_columns = [\"alt_market_cap\", \"alt_dominance_roc_30d\"]\n",
    "plot_columns(new_master_df, columns_to_plot, secondary_y_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "cmc_fgindex = pd.DataFrame(\n",
    "    requests.get(\n",
    "        f\"https://api.coinmarketcap.com/data-api/v3/fear-greed/chart?start=1356978600&end={int(datetime.now().timestamp())}\"\n",
    "    ).json()[\"data\"][\"dataList\"]\n",
    ")\n",
    "\n",
    "cmc_fgindex[\"timestamp\"] = pd.to_datetime(cmc_fgindex[\"timestamp\"], unit=\"s\")\n",
    "\n",
    "cmc_fgindex = cmc_fgindex.set_index(\"timestamp\").sort_index()\n",
    "\n",
    "cmc_fgindex\n",
    "\n",
    "# %%\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_fear_greed_data(save_path=\"fear_greed_history.csv\"):\n",
    "    \"\"\"\n",
    "    Fetch and maintain Fear & Greed Index data with proper handling of\n",
    "    daily historical and hourly recent data.\n",
    "\n",
    "    Parameters:\n",
    "    save_path (str): Path to save/load the csv file\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Complete Fear & Greed Index history\n",
    "    \"\"\"\n",
    "\n",
    "    def fetch_raw_data():\n",
    "        \"\"\"Fetch raw Fear & Greed Index data\"\"\"\n",
    "        response = requests.get(\n",
    "            f\"https://api.coinmarketcap.com/data-api/v3/fear-greed/chart?start=1356978600&end={int(datetime.now().timestamp())}\"\n",
    "        ).json()\n",
    "\n",
    "        df = pd.DataFrame(response[\"data\"][\"dataList\"])\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"s\")\n",
    "        dp(df)\n",
    "        return df.set_index(\"timestamp\").sort_index()\n",
    "\n",
    "    def find_hourly_entry(df):\n",
    "        \"\"\"Find the last entry which should be hourly\"\"\"\n",
    "        time_diffs = df.index.to_series().diff()\n",
    "        hourly_entries = time_diffs[time_diffs < pd.Timedelta(days=1)]\n",
    "        return hourly_entries.index[-1] if not hourly_entries.empty else None\n",
    "\n",
    "    def create_initial_file():\n",
    "        \"\"\"Create initial file with historical daily data (excluding latest hourly)\"\"\"\n",
    "        df = fetch_raw_data()\n",
    "\n",
    "        # Exclude the last hourly entry\n",
    "        df = df[:-1]\n",
    "\n",
    "        # Round to daily for historical data\n",
    "        df.index = df.index.round(\"D\")\n",
    "\n",
    "        # Remove any duplicates after rounding\n",
    "        df = df[~df.index.duplicated(keep=\"last\")].sort_index()\n",
    "        return df\n",
    "\n",
    "    def update_data(existing_df):\n",
    "        \"\"\"Update with latest data, maintaining daily history and adding new days\"\"\"\n",
    "        new_df = fetch_raw_data()\n",
    "\n",
    "        try:\n",
    "            # Get the last hourly entry\n",
    "            hourly_entry = find_hourly_entry(new_df)\n",
    "\n",
    "            if hourly_entry is None:\n",
    "                print(\"No hourly data found in new data\")\n",
    "                return existing_df\n",
    "\n",
    "            # Split into historical and latest\n",
    "            historical_data = new_df[:hourly_entry].copy()\n",
    "            latest_entry = new_df[hourly_entry:].copy()\n",
    "\n",
    "            # Round historical to daily\n",
    "            historical_data.index = historical_data.index.round(\"D\")\n",
    "            historical_data = historical_data[~historical_data.index.duplicated(keep=\"last\")]\n",
    "\n",
    "            # Combine existing and new historical data\n",
    "            combined_historical = pd.concat([existing_df, historical_data])\n",
    "            combined_historical = combined_historical[~combined_historical.index.duplicated(keep=\"last\")]\n",
    "\n",
    "            # Add the latest hourly entry\n",
    "            updated_df = pd.concat([combined_historical, latest_entry])\n",
    "            updated_df = updated_df[~updated_df.index.duplicated(keep=\"last\")].sort_index()\n",
    "\n",
    "            return updated_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating data: {e}\")\n",
    "            return existing_df\n",
    "\n",
    "    file_path = Path(save_path)\n",
    "\n",
    "    if not file_path.exists():\n",
    "        # Initial file creation\n",
    "        print(\"Creating new Fear & Greed Index database...\")\n",
    "        df = create_initial_file()\n",
    "        df.to_csv(file_path)\n",
    "        return df\n",
    "\n",
    "    # Update existing file\n",
    "    print(\"Updating Fear & Greed Index data...\")\n",
    "    existing_df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "    updated_df = update_data(existing_df)\n",
    "\n",
    "    # Save if there are changes\n",
    "    if not updated_df.equals(existing_df):\n",
    "        print(\"New data found, saving updates...\")\n",
    "        updated_df.to_csv(file_path)\n",
    "    else:\n",
    "        print(\"No new data to update\")\n",
    "\n",
    "    return updated_df\n",
    "\n",
    "    # Example usage:\n",
    "\n",
    "\n",
    "fg_df = get_fear_greed_data()\n",
    "fg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_two_consecutive_nonnan(column):\n",
    "    # Shift the column by 1\n",
    "    shifted_column = column.shift(-1)\n",
    "\n",
    "    same_value = column == shifted_column\n",
    "    same_value = same_value[same_value == True]\n",
    "\n",
    "    return same_value\n",
    "\n",
    "    # # Find indices where both current and next values are not NaN\n",
    "    # non_nan_indices = column.notna() & shifted_column.notna()\n",
    "\n",
    "    # # Get the first index where the condition is True\n",
    "    # first_valid_index = non_nan_indices.idxmax() if non_nan_indices.any() else None\n",
    "\n",
    "    # return first_valid_index\n",
    "\n",
    "\n",
    "for token in token_data_df.columns:\n",
    "    dis(token, first_two_consecutive_nonnan(token_data_df[token]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
